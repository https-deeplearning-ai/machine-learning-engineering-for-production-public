{"cells":[{"cell_type":"markdown","metadata":{"id":"MfBg1C5NB3X0"},"source":["# Ungraded lab: Distributed Strategies with TF and Keras\n","------------------------\n"]},{"cell_type":"markdown","metadata":{"id":"8yL0-KcLPwtk"},"source":["\n","Welcome, during this ungraded lab you are going to perform a distributed training strategy using TensorFlow and Keras, specifically the [`tf.distribute.MultiWorkerMirroredStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/MultiWorkerMirroredStrategy). \n","\n","With the help of this strategy, a Keras model that was designed to run on single-worker can seamlessly work on multiple workers with minimal code change. In particular you will:\n","\n","\n","1. Perform training with a single worker.\n","2. Understand the requirements for a multi-worker setup (`tf_config` variable) and using context managers for implementing distributed strategies.\n","3. Use magic commands to simulate different machines.\n","4. Perform a multi-worker training strategy.\n","\n","This notebook is based on the official [Multi-worker training with Keras](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras) notebook, which covers some additional topics in case you want a deeper dive into this topic.\n","\n","[Distributed Training with TensorFlow](https://www.tensorflow.org/guide/distributed_training) guide is also available for an overview of the distribution strategies TensorFlow supports for those interested in a deeper understanding of `tf.distribute.Strategy` APIs.\n","\n","Let's get started!"]},{"cell_type":"markdown","metadata":{"id":"MUXex9ctTuDB"},"source":["## Setup\n","\n","First, some necessary imports."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bnYxvfLD-LW-","vscode":{"languageId":"python"}},"outputs":[],"source":["import os\n","import sys\n","import json\n","import time\n","\n","# Log additional outputs from TF's C++ backend\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0'"]},{"cell_type":"markdown","metadata":{"id":"Zz0EY91y3mxy"},"source":["Before importing TensorFlow, make a few changes to the environment.\n","\n","- Disable all GPUs. This prevents errors caused by the workers all trying to use the same GPU. **For a real application each worker would be on a different machine.**\n","\n","\n","- Add the current directory to python's path so modules in this directory can be imported."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"685pbYEY3jGC","vscode":{"languageId":"python"}},"outputs":[],"source":["# Disable GPUs\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n","\n","# Add current directory to path\n","if '.' not in sys.path:\n","  sys.path.insert(0, '.')"]},{"cell_type":"markdown","metadata":{"id":"Rd4L9Ii77SS8"},"source":["The previous step is important since this notebook relies on writting files using the magic command `%%writefile` and then importing them as modules.\n","\n","Now that the environment configuration is ready, import TensorFlow.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vHNvttzV43sA","vscode":{"languageId":"python"}},"outputs":[],"source":["import tensorflow as tf\n","\n","# Ignore warnings\n","tf.get_logger().setLevel('ERROR')"]},{"cell_type":"markdown","metadata":{"id":"0S2jpf6Sx50i"},"source":["### Dataset and model definition"]},{"cell_type":"markdown","metadata":{"id":"fLW6D2TzvC-4"},"source":["Next create an `mnist.py` file with a simple model and dataset setup. This python file will be used by the worker-processes in this tutorial.\n","\n","The name of this file derives from the dataset you will be using which is called [mnist](https://keras.io/api/datasets/mnist/) and consists of 60,000 28x28 grayscale images of the first 10 digits."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dma_wUAxZqo2","vscode":{"languageId":"python"}},"outputs":[],"source":["%%writefile mnist.py\n","\n","# import os\n","import tensorflow as tf\n","import numpy as np\n","\n","def mnist_dataset(batch_size):\n","  # Load the data\n","  (x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n","  # Normalize pixel values for x_train and cast to float32\n","  x_train = x_train / np.float32(255)\n","  # Cast y_train to int64\n","  y_train = y_train.astype(np.int64)\n","  # Define repeated and shuffled dataset\n","  train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(60000).repeat().batch(batch_size)\n","  return train_dataset\n","\n","\n","def build_and_compile_cnn_model():\n","  # Define simple CNN model using Keras Sequential\n","  model = tf.keras.Sequential([\n","      tf.keras.layers.InputLayer(input_shape=(28, 28)),\n","      tf.keras.layers.Reshape(target_shape=(28, 28, 1)),\n","      tf.keras.layers.Conv2D(32, 3, activation='relu'),\n","      tf.keras.layers.Flatten(),\n","      tf.keras.layers.Dense(128, activation='relu'),\n","      tf.keras.layers.Dense(10)\n","  ])\n","\n","  # Compile model\n","  model.compile(\n","      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","      optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\n","      metrics=['accuracy'])\n","  \n","  return model"]},{"cell_type":"markdown","metadata":{"id":"CbDEKpGowcyT"},"source":["Check that the file was succesfully created:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IxsnfpVurQ1g","vscode":{"languageId":"python"}},"outputs":[],"source":["!ls *.py"]},{"cell_type":"markdown","metadata":{"id":"2UL3kisMO90X"},"source":["Import the mnist module you just created and try training the model for a small number of epochs to observe the results of a single worker to make sure everything works correctly."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6Qe6iAf5O8iJ","vscode":{"languageId":"python"}},"outputs":[],"source":["# Import your mnist model\n","import mnist\n","\n","# Set batch size\n","batch_size = 64\n","\n","# Load the dataset\n","single_worker_dataset = mnist.mnist_dataset(batch_size)\n","\n","# Load compiled CNN model\n","single_worker_model = mnist.build_and_compile_cnn_model()\n","\n","# As training progresses, the loss should drop and the accuracy should increase.\n","single_worker_model.fit(single_worker_dataset, epochs=3, steps_per_epoch=70)"]},{"cell_type":"markdown","metadata":{"id":"ZpLPWFJh1CAK"},"source":["Everything is working as expected! \n","\n","Now you will see how multiple workers can be used as a distributed strategy."]},{"cell_type":"markdown","metadata":{"id":"JmgZwwymxqt5"},"source":["## Multi-worker Configuration\n","\n","Now let's enter the world of multi-worker training. In TensorFlow, the `TF_CONFIG` environment variable is required for training on multiple machines, each of which possibly has a different role. `TF_CONFIG` is a JSON string used to specify the cluster configuration on each worker that is part of the cluster.\n","\n","There are two components of `TF_CONFIG`: `cluster` and `task`. \n","\n","Let's dive into how they are used:\n","\n","`cluster`:\n","- **It is the same for all workers** and provides information about the training cluster, which is a dict consisting of different types of jobs such as `worker`.\n","\n","- In multi-worker training with `MultiWorkerMirroredStrategy`, there is usually one `worker` that takes on a little more responsibility like saving checkpoint and writing summary file for TensorBoard in addition to what a regular `worker` does. \n","-Such a worker is referred to as the `chief` worker, and it is customary that the `worker` with `index` 0 is appointed as the chief `worker` (in fact this is how `tf.distribute.Strategy` is implemented).\n","\n","`task`:\n","- Provides information of the current task and is different on each worker. It specifies the `type` and `index` of that worker. \n","\n","Here is an example configuration:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XK1eTYvSZiX7","vscode":{"languageId":"python"}},"outputs":[],"source":["tf_config = {\n","    'cluster': {\n","        'worker': ['localhost:12345', 'localhost:23456']\n","    },\n","    'task': {'type': 'worker', 'index': 0}\n","}"]},{"cell_type":"markdown","metadata":{"id":"JjgwJbPKZkJL"},"source":["Here is the same `TF_CONFIG` serialized as a JSON string:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yY-T0YDQZjbu","vscode":{"languageId":"python"}},"outputs":[],"source":["json.dumps(tf_config)"]},{"cell_type":"markdown","metadata":{"id":"8YFpxrcsZ2xG"},"source":["### Explaining the TF_CONFIG example\n","\n","In this example you set a `TF_CONFIG` with 2 workers on `localhost`. In practice, users would create multiple workers on external IP addresses/ports, and set `TF_CONFIG` on each worker appropriately.\n","\n","Since you set the task `type` to `\"worker\"` and the task `index` to `0`, **this machine is the first worker and will be appointed as the chief worker**. \n","\n","Note that other machines will need to have the `TF_CONFIG` environment variable set as well, and it should have the same `cluster` dict, but different task `type` or task `index` depending on what the roles of those machines are. For instance, for the second worker you would set `tf_config['task']['index']=1`.\n"]},{"cell_type":"markdown","metadata":{"id":"f83FVYqDX3aX"},"source":["### Quick Note on Environment variables and subprocesses in notebooks\n","\n","Above, `tf_config` is just a local variable in python. To actually use it to configure training, this dictionary needs to be serialized as JSON, and placed in the `TF_CONFIG` environment variable.\n","\n","In the next section, you'll spawn new subprocesses for each worker using the `%%bash` magic command. Subprocesses inherit environment variables from their parent, so they can access `TF_CONFIG`. \n","\n","You would never really launch your jobs this way (as subprocesses of an interactive Python runtime), but it's how you will do it for the purposes of this tutorial."]},{"cell_type":"markdown","metadata":{"id":"UhNtHfuxCGVy"},"source":["## Choose the right strategy\n","\n","In TensorFlow there are two main forms of distributed training:\n","\n","* Synchronous training, where the steps of training are synced across the workers and replicas, and\n","* Asynchronous training, where the training steps are not strictly synced.\n","\n","`MultiWorkerMirroredStrategy`, which is the recommended strategy for synchronous multi-worker training is the one you will be using.\n","\n","To train the model, use an instance of `tf.distribute.MultiWorkerMirroredStrategy`.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1uFSHCJXMrQ-","vscode":{"languageId":"python"}},"outputs":[],"source":["strategy = tf.distribute.MultiWorkerMirroredStrategy()"]},{"cell_type":"markdown","metadata":{"id":"N0iv7SyyAohc"},"source":["`MultiWorkerMirroredStrategy` creates copies of all variables in the model's layers on each device across all workers.  It uses `CollectiveOps`, a TensorFlow op for collective communication, to aggregate gradients and keep the variables in sync.  The [official TF distributed training guide](https://www.tensorflow.org/guide/distributed_training) has more details about this.\n"]},{"cell_type":"markdown","metadata":{"id":"H47DDcOgfzm7"},"source":["### Implement Distributed Training via Context Managers\n","\n","To distribute the training to multiple-workers all you need to do is to enclose the model building and `model.compile()` call inside `strategy.scope()`. \n","\n","The distribution strategy's scope dictates how and where the variables are created, and in the case of `MultiWorkerMirroredStrategy`, the variables created are `MirroredVariable`s, and they are replicated on each of the workers.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wo6b9wX65glL","vscode":{"languageId":"python"}},"outputs":[],"source":["# Implementing distributed strategy via a context manager\n","with strategy.scope():\n","  multi_worker_model = mnist.build_and_compile_cnn_model()"]},{"cell_type":"markdown","metadata":{"id":"jfYpmIxO6Jck"},"source":["Note: `TF_CONFIG` is parsed and TensorFlow's GRPC servers are started at the time `MultiWorkerMirroredStrategy()` is called, so the `TF_CONFIG` environment variable must be set before a `tf.distribute.Strategy` instance is created. \n","\n","**Since `TF_CONFIG` is not set yet the above strategy is effectively single-worker training**.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"JxzYhF0dL6qQ"},"source":["## Train the model\n","\n","### Create training script\n","\n","To actually run with `MultiWorkerMirroredStrategy` you'll need to run worker processes and pass a `TF_CONFIG` to them.\n","\n","Like the `mnist.py` file written earlier, here is the `main.py` that each of the workers will run:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BcsuBYrpgnlS","vscode":{"languageId":"python"}},"outputs":[],"source":["%%writefile main.py\n","\n","import os\n","import json\n","\n","import tensorflow as tf\n","import mnist # Your module\n","\n","# Define batch size\n","per_worker_batch_size = 64\n","\n","# Get TF_CONFIG from the env variables and save it as JSON\n","tf_config = json.loads(os.environ['TF_CONFIG'])\n","\n","# Infer number of workers from tf_config\n","num_workers = len(tf_config['cluster']['worker'])\n","\n","# Define strategy\n","strategy = tf.distribute.MultiWorkerMirroredStrategy()\n","\n","# Define global batch size\n","global_batch_size = per_worker_batch_size * num_workers\n","\n","# Load dataset\n","multi_worker_dataset = mnist.mnist_dataset(global_batch_size)\n","\n","# Create and compile model following the distributed strategy\n","with strategy.scope():\n","  multi_worker_model = mnist.build_and_compile_cnn_model()\n","\n","# Train the model\n","multi_worker_model.fit(multi_worker_dataset, epochs=3, steps_per_epoch=70)"]},{"cell_type":"markdown","metadata":{"id":"Aom9xelvJQ_6"},"source":["In the code snippet above note that the `global_batch_size`, which gets passed to `Dataset.batch`, is set to `per_worker_batch_size * num_workers`. This ensures that each worker processes batches of `per_worker_batch_size` examples regardless of the number of workers."]},{"cell_type":"markdown","metadata":{"id":"lHLhOii67Saa"},"source":["The current directory should now contain both Python files:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bi6x05Sr60O9","vscode":{"languageId":"python"}},"outputs":[],"source":["!ls *.py"]},{"cell_type":"markdown","metadata":{"id":"qmEEStPS6vR_"},"source":["### Set TF_CONFIG environment variable\n","\n","Now json-serialize the `TF_CONFIG` and add it to the environment variables:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9uu3g7vV7Bbt","vscode":{"languageId":"python"}},"outputs":[],"source":["# Set TF_CONFIG env variable\n","os.environ['TF_CONFIG'] = json.dumps(tf_config)"]},{"cell_type":"markdown","metadata":{"id":"-WDqwMPNneON"},"source":["And terminate all background processes:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"txMXaq8d8N_S","vscode":{"languageId":"python"}},"outputs":[],"source":["# first kill any previous runs\n","%killbgscripts"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Before launching the first worker you can check that port `12345` is free at the time:"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# This should not print anything at the moment\n","!lsof -i :12345"]},{"cell_type":"markdown","metadata":{"id":"MsY3dQLK7jdf"},"source":["### Launch the first worker\n","\n","Now, you can launch a worker process that will run the `main.py` and use the `TF_CONFIG`:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qnSma_Ck7r-r","vscode":{"languageId":"python"}},"outputs":[],"source":["%%bash --bg\n","python main.py &> job_0.log"]},{"cell_type":"markdown","metadata":{"id":"ZChyazqS7v0P"},"source":["There are a few things to note about the above command:\n","\n","1. It uses the `%%bash` which is a [notebook \"magic\"](https://ipython.readthedocs.io/en/stable/interactive/magics.html) to run some bash commands.\n","2. It uses the `--bg` flag to run the `bash` process in the background, because this worker will not terminate. It waits for all the workers before it starts.\n","\n","The backgrounded worker process won't print output to this notebook, so the `&>` redirects its output to a file, so you can see what happened.\n","\n","So, wait a few seconds for the process to start up:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hm2yrULE9281","vscode":{"languageId":"python"}},"outputs":[],"source":["# Wait for logs to be written to the file\n","time.sleep(10)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Now you can check again at the status of port `12345`:"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["!lsof -i :12345"]},{"cell_type":"markdown","metadata":{"id":"ZFPoNxg_9_Mx"},"source":["Now look what's been output to the worker's logfile so far using the `cat` command:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vZEOuVgQ9-hn","vscode":{"languageId":"python"}},"outputs":[],"source":["%%bash\n","cat job_0.log"]},{"cell_type":"markdown","metadata":{"id":"RqZhVF7L_KOy"},"source":["The last line of the log file should say: `Started server with target: grpc://localhost:12345`. The first worker is now ready, and is waiting for all the other worker(s) to be ready to proceed."]},{"cell_type":"markdown","metadata":{"id":"Pi8vPNNA_l4a"},"source":["### Launch the second worker\n","\n","Now update the `tf_config` for the second worker's process to pick up:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lAiYkkPu_Jqd","vscode":{"languageId":"python"}},"outputs":[],"source":["tf_config['task']['index'] = 1\n","os.environ['TF_CONFIG'] = json.dumps(tf_config)"]},{"cell_type":"markdown","metadata":{"id":"0AshGVO0_x0w"},"source":["Now launch the second worker. This will start the training since all the workers are active (so there's no need to background this process):"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_ESVtyQ9_xjx","vscode":{"languageId":"python"}},"outputs":[],"source":["%%bash\n","python main.py"]},{"cell_type":"markdown","metadata":{"id":"hX4FA2O2AuAn"},"source":["Now if you recheck the logs written by the first worker you'll see that it participated in training that model:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rc6hw3yTBKXX","vscode":{"languageId":"python"}},"outputs":[],"source":["%%bash\n","cat job_0.log"]},{"cell_type":"markdown","metadata":{"id":"zL79ak5PMzEg"},"source":["Unsurprisingly this ran _slower_ than the the test run at the beginning of this tutorial. **Running multiple workers on a single machine only adds overhead**. The goal here was not to improve the training time, but only to give an example of multi-worker training."]},{"cell_type":"markdown","metadata":{"id":"xckY28bOV_p8"},"source":["-----------------------------\n","**Congratulations on finishing this ungraded lab!** Now you should have a clearer understanding of how to implement distributed strategies with Tensorflow and Keras. \n","\n","Although this tutorial didn't show the true power of a distributed strategy since this will require multiple machines operating under the same network, you now know how this process looks like at a high level. \n","\n","In practice and especially with very big models, distributed strategies are commonly used as they provide a way of better managing resources to perform time-consuming tasks, such as training in a fraction of the time that it will take without the strategy.\n","\n","**Keep it up!**"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"C3W3_Colab_Lab1_Distributed_Training.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}
