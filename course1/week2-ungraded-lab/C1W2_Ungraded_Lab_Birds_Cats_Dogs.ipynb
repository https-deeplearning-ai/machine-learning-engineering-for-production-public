{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kF6XR0AMAuRW"
   },
   "source": [
    "# Week 2 - Ungraded Lab: A Journey through Data\n",
    "\n",
    "The paradigm behind machine learning is shifting from model-centric to data-centric. In this lab, you will see how data preparation affects the output of your models. You will build an image classifier that detects three classes: dog, cat, and bird. To demonstrate the effect of your input data, you will only use a single model throughout -- a simple Convolutional Neural Network (CNN) -- and will only vary the dataset. You will navigate common problems such as class imbalance and overfitting, and will walk through useful diagnosis tools and methods to mitigate these issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lao0CVv7c3Rd"
   },
   "source": [
    "We have provided pretrained models for this lab to help you focus on observation and reflection about data-centric AI. However, if you are interested in training the models yourself, we have included optional instructions to do so. **If you're interested in training the models yourself,** please read the notes below.\n",
    "\n",
    "<details>\n",
    "  <summary><font size=\"3\" color=\"blue\"><b>Click here for important notes regarding training the models in this lab</b></font></summary>\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "**NOTE 1:**\n",
    "There are a total of 3 CNNs that require training and although some parameters have been tuned to provide a faster training time, this may result in a long time spent running this lab rather than thinking about what you observe.\n",
    "\n",
    "To speed things up, we have provided pre-trained versions of each model along with their respective training history. We recommend you use these pre-trained versions to save time. If you choose this option, you can stay here in the Coursera Lab environment to run the entire lab.\n",
    "\n",
    "**However, if you want to perform the model training yourself, the code for replicating the training is provided as well. In this case, a GPU is absolutely necessary. The Coursera Lab environment only runs on CPU so you need to use another environment such as Colab to train the models.**\n",
    "\n",
    "You can open this lab on Colab by clicking on the badge below:\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/https-deeplearning-ai/machine-learning-engineering-for-production-public/blob/version_3/course1/week2-ungraded-lab/C1W2_Ungraded_Lab_Birds_Cats_Dogs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "To make sure your runtime is GPU, you can select `Runtime -> Change runtime type -> T4 GPU` from the menu and wait for the runtime to reconnect (if it's not already selected).\n",
    "\n",
    "\n",
    "**NOTE 2:**\n",
    "\n",
    "Colab **does not** guarantee access to a GPU. This depends on the availability of these resources. However **it is not very common to be denied GPU access**. If this happens to you, remember that you can still run this lab without training the models yourself. If you really want to do the training but are denied a GPU, try switching the runtime to a GPU after a couple of hours.\n",
    "\n",
    "To know more about Colab's policies check out this [FAQ](https://research.google.com/colaboratory/faq.html).\n",
    "\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v4Gq9Xffccwt"
   },
   "source": [
    "## Lab Setup\n",
    "\n",
    "Start by downloading the dataset. You will be using a combination of the [Cats vs. Dogs](https://www.microsoft.com/en-us/download/details.aspx?id=54765) and [Caltech Birds](https://www.vision.caltech.edu/datasets/cub_200_2011/) datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "umT5uBD3Ziix"
   },
   "outputs": [],
   "source": [
    "# Install compatible Tensorflow version\n",
    "!pip install tensorflow==2.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CkTzJYihXWu3"
   },
   "outputs": [],
   "source": [
    "# Download the dataset\n",
    "!wget -nc https://storage.googleapis.com/mlep-public/course_1/week2/cats_dogs_birds.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ySR9dlWJZK3e"
   },
   "source": [
    "You will also download a `lab_utils.py` file. This file contains helper functions for this lab. Feel free to explore this file if you want to see how they are implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zdILM4cEZK3f"
   },
   "outputs": [],
   "source": [
    "# Download lab utilities\n",
    "!wget -nc https://storage.googleapis.com/mlep-public/course_1/week2/lab_utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvaDa_6ZZK3f"
   },
   "source": [
    "Next, you will also need to download the pre-trained models and histories below. These will allow you to see sample results without training the models yourself. (If you intend to train the models yourself in an environment with a GPU, you can skip this step.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "heuBWjQOZK3f"
   },
   "outputs": [],
   "source": [
    "# Download pretrained models and training histories\n",
    "!wget -q -nc -P ./models/ https://storage.googleapis.com/mlep-public/course_1/week2/v2/imbalanced_model.tar.gz\n",
    "!wget -q -nc -P ./models/ https://storage.googleapis.com/mlep-public/course_1/week2/v2/balanced_model.tar.gz\n",
    "!wget -q -nc -P ./models/ https://storage.googleapis.com/mlep-public/course_1/week2/v2/augmented_model.tar.gz\n",
    "!wget -q -nc -P ./histories/ https://storage.googleapis.com/mlep-public/course_1/week2/v2/imbalanced_history.pkl\n",
    "!wget -q -nc -P ./histories/ https://storage.googleapis.com/mlep-public/course_1/week2/v2/balanced_history.pkl\n",
    "!wget -q -nc -P ./histories/ https://storage.googleapis.com/mlep-public/course_1/week2/v2/augmented_history.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "namXPDHiZK3f"
   },
   "source": [
    "You will now import the packages needed in this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LttdbzB5XB0O"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, balanced_accuracy_score\n",
    "import lab_utils\n",
    "import tensorflow as tf\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YWMc3o1WZK3g"
   },
   "source": [
    "You will also define some global variables that you will use throughout the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rAWcp2qzZK3g"
   },
   "outputs": [],
   "source": [
    "# Filename of the downloaded dataset archive\n",
    "DATASET_COMPRESSED = './cats_dogs_birds.tar.gz'\n",
    "\n",
    "# Base directory for extracting and preparing the dataset\n",
    "DATA_DIR = '/tmp/data'\n",
    "\n",
    "# Base directory for extracting the pretrained models\n",
    "MODEL_DIR = './models'\n",
    "\n",
    "# Name of the classes to predict\n",
    "ANIMALS = ['dogs', 'cats', 'birds']\n",
    "\n",
    "# Directories for the training and dev sets\n",
    "TRAIN_DIRS = ['train/dogs', 'train/cats', 'train/birds']\n",
    "DEV_DIRS = ['dev/dogs', 'dev/cats', 'dev/birds']\n",
    "\n",
    "# Imbalanced portion of images among the 3 classes\n",
    "PORTIONS = [0.2, 1, 0.1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qZPkviieZK3g"
   },
   "source": [
    "Next, run the cell below to extract the pretrained models. If you intend to train the models yourself in an environment with a GPU, you can skip this step. (Note: If you're running this on Coursera, the models are already pre-extracted on your environment so you can skip this step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aXTrc1CnZK3g"
   },
   "outputs": [],
   "source": [
    "# # Extract the compressed models (Not needed in the Coursera environment)\n",
    "# compressed_models = ['imbalanced_model.tar.gz', 'balanced_model.tar.gz', 'augmented_model.tar.gz']\n",
    "\n",
    "# for compressed_model in compressed_models:\n",
    "#     with tarfile.open(f'{MODEL_DIR}/{compressed_model}', 'r') as my_tar:\n",
    "#       my_tar.extractall(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "suKuIsOYdC9G"
   },
   "source": [
    "## A story of data\n",
    "\n",
    "Throughout this lab, you will simulate a real life scenario:\n",
    "\n",
    "You have been tasked with creating a model that classifies images of cats, dogs and birds. To establish a baseline, you settle on a simple Convolutional Neural Network (CNN) architecture since CNNs are known to perform well on image classification tasks. You thought of using two widely used datasets: `cats vs dogs`, and `caltech birds` to train your model.\n",
    "\n",
    "_Note: Both datasets are also available through [TensforFlow Datasets (TFDS)](https://github.com/tensorflow/datasets). For this lab however, you will prepare the datasets yourself so that you have more control over the manipulations._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xRqfAVn6e8Lp"
   },
   "source": [
    "You downloaded the compressed dataset earlier and the next step is extracting it to a base directory. This usually takes a few seconds, but sometimes up to a minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aUl3_4nVXcsE"
   },
   "outputs": [],
   "source": [
    "# Extract the dataset\n",
    "with tarfile.open(DATASET_COMPRESSED, 'r') as my_tar:\n",
    "  my_tar.extractall(DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WYkn9KLJZK3h"
   },
   "source": [
    "The dataset has an `images` top-level directory and has 3 subdirectories for each class: `dog`, `cat`, and `bird`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N525paXCZK3h"
   },
   "outputs": [],
   "source": [
    "os.listdir(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qATF00Q5ZK3h"
   },
   "outputs": [],
   "source": [
    "os.listdir(f'{DATA_DIR}/images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "65E3t5Qlfwwn"
   },
   "source": [
    "You will assign these to variables and check how many images are in each subdirectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "husRshAjYim9"
   },
   "outputs": [],
   "source": [
    "base_dogs_dir = os.path.join(DATA_DIR, 'images/dog')\n",
    "base_cats_dir = os.path.join(DATA_DIR,'images/cat')\n",
    "base_birds_dir = os.path.join(DATA_DIR,'images/bird')\n",
    "base_image_dirs = [base_dogs_dir, base_cats_dir, base_birds_dir]\n",
    "\n",
    "for animal, base_image_dir in zip(ANIMALS, base_image_dirs):\n",
    "    print(f\"There are {len(os.listdir(base_image_dir))} images of {animal}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9tteiK1fieHo"
   },
   "source": [
    "It turns out that there is a similar number of images for each class you are trying to predict. This will likely lead to your model having the same performance on all three classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z3jHPdb7SE61"
   },
   "source": [
    "Take a quick look at an image from each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lXE9RlF2ZFLL"
   },
   "outputs": [],
   "source": [
    "print(\"Sample cat image:\")\n",
    "display(Image(filename=f\"{os.path.join(base_cats_dir, os.listdir(base_cats_dir)[0])}\"))\n",
    "print(\"\\nSample dog image:\")\n",
    "display(Image(filename=f\"{os.path.join(base_dogs_dir, os.listdir(base_dogs_dir)[0])}\"))\n",
    "print(\"\\nSample bird image:\")\n",
    "display(Image(filename=f\"{os.path.join(base_birds_dir, os.listdir(base_birds_dir)[0])}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FduWstcripzJ"
   },
   "source": [
    "## Train / Dev Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EiL9L8eSizCp"
   },
   "source": [
    "Before training the model, you need to split the data into training and dev sets. For training, you will use the [`Keras`](https://keras.io) application programming interface (API) which includes functionality to read images from  various directories.\n",
    "\n",
    "Run the next cell to create the directories for training and dev sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NdBnzB2Mvcs2"
   },
   "outputs": [],
   "source": [
    "for dir in TRAIN_DIRS:\n",
    "    os.makedirs(os.path.join(DATA_DIR, dir), exist_ok=True)\n",
    "\n",
    "for dir in DEV_DIRS:\n",
    "    os.makedirs(os.path.join(DATA_DIR, dir), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bLs-A-SXZK3i"
   },
   "source": [
    "Notice that there are now `train` and `dev` directories in the base data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R4fZvoGRZK3i"
   },
   "outputs": [],
   "source": [
    "os.listdir(DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SRpMvBd1ZK3i"
   },
   "source": [
    "Moreover, these have subdirectories for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DoB-5_UxZK3i"
   },
   "outputs": [],
   "source": [
    "os.listdir(os.path.join(DATA_DIR, 'train'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iiCF3bYnZK3i"
   },
   "outputs": [],
   "source": [
    "os.listdir(os.path.join(DATA_DIR, 'dev'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x4XYN51Zj7-J"
   },
   "source": [
    "You will move a percentage of images from an origin folder to a destination folder as desired to generate the training and dev splits. You will use the `move_to_destination()` function from `lab_utils` for this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VMKvQGH6fGdW"
   },
   "outputs": [],
   "source": [
    "# Move 70% of the images to the training directories\n",
    "for base_image_dir, train_dir in zip(base_image_dirs, TRAIN_DIRS):\n",
    "    lab_utils.move_to_destination(\n",
    "        base_image_dir,\n",
    "        os.path.join(DATA_DIR, train_dir),\n",
    "        0.7\n",
    "    )\n",
    "\n",
    "\n",
    "# Move the remaining images to the eval directories\n",
    "for base_image_dir, dev_dir in zip(base_image_dirs, DEV_DIRS):\n",
    "    lab_utils.move_to_destination(\n",
    "        base_image_dir,\n",
    "        os.path.join(DATA_DIR, dev_dir),\n",
    "        1\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oeqbprKcmr-0"
   },
   "source": [
    "Check how many images you have available for each split and class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nZFk4f0jhEAk"
   },
   "outputs": [],
   "source": [
    "for animal, train_dir in zip(ANIMALS, TRAIN_DIRS):\n",
    "    print(f\"There are {len(os.listdir(os.path.join(DATA_DIR, train_dir)))} images of {animal} for training\")\n",
    "\n",
    "print()\n",
    "\n",
    "for animal, dev_dir in zip(ANIMALS, DEV_DIRS):\n",
    "    print(f\"There are {len(os.listdir(os.path.join(DATA_DIR, dev_dir)))} images of {animal} for evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LSmRaN_Qm-s4"
   },
   "source": [
    "## An unexpected issue!\n",
    "\n",
    "Let's face the first real life issue in this narrative! There was a power outage in your office and some hard drives were damaged. As a result of the damage, many of the images for `dogs` and `birds` have been erased. As a matter of fact, only 20% of the dog images and 10% of the bird images survived.\n",
    "\n",
    "To simulate this scenario, create a new directory called `imbalanced` and copy only the portions mentioned above for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wAG-rJRPZTQt"
   },
   "outputs": [],
   "source": [
    "for dir in TRAIN_DIRS + DEV_DIRS:\n",
    "    os.makedirs(os.path.join(DATA_DIR, 'imbalanced/'+dir), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GAGTj51qZT4e"
   },
   "outputs": [],
   "source": [
    "# Perform the copying\n",
    "for train_dir, portion in zip(TRAIN_DIRS, PORTIONS):\n",
    "    lab_utils.copy_with_limit(\n",
    "        os.path.join(DATA_DIR, train_dir),\n",
    "        os.path.join(DATA_DIR, f'imbalanced/{train_dir}'),\n",
    "        portion\n",
    "    )\n",
    "\n",
    "for dev_dir, portion in zip(DEV_DIRS, PORTIONS):\n",
    "    lab_utils.copy_with_limit(\n",
    "        os.path.join(DATA_DIR, dev_dir),\n",
    "        os.path.join(DATA_DIR, f'imbalanced/{dev_dir}'),\n",
    "        portion\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eoT74Wj3ZK3k"
   },
   "source": [
    "Now, check the images for each split and class again. You'll notice that the images are now imbalanced, where the cat pictures dominate the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r7w-igjrZK3k"
   },
   "outputs": [],
   "source": [
    "# Print number of available images\n",
    "for animal, train_dir in zip(ANIMALS, TRAIN_DIRS):\n",
    "    print(f\"There are {len(os.listdir(os.path.join(DATA_DIR, f'imbalanced/{train_dir}')))} images of {animal} for training\")\n",
    "\n",
    "print()\n",
    "\n",
    "for animal, dev_dir in zip(ANIMALS, DEV_DIRS):\n",
    "    print(f\"There are {len(os.listdir(os.path.join(DATA_DIR, f'imbalanced/{dev_dir}')))} images of {animal} for evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Qt_EGGJAaOR"
   },
   "source": [
    "For now there is no quick or clear solution to remedy the accidental file loss. You decide to keep going and train the model with the remaining images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qlDuR43ZAfwk"
   },
   "source": [
    "## Selecting the model\n",
    "\n",
    "Use the `lab_utils.create_model()` function to create a model and define a loss function, optimizer and performance metrics leveraging the Keras API. You can print out the model summary as a quick check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "elM3J9P8I_zu"
   },
   "outputs": [],
   "source": [
    "# Create a model to use with the imbalanced dataset\n",
    "imbalanced_model = lab_utils.create_model()\n",
    "\n",
    "# Print the model's summary\n",
    "print(imbalanced_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9YjjV9iU78Ca"
   },
   "source": [
    "## Preparing the Data\n",
    "\n",
    "You will use the [tf.data API](https://www.tensorflow.org/guide/data) to prepare the datasets so it can be consumed by the model.\n",
    "\n",
    "The [image_dataset_from_directory() utility](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image_dataset_from_directory) allows reading images from a base directory and outputs a [tf.data.Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset). You will pass in the following arguments:\n",
    "\n",
    "- `directory`: Path to the root directory where the images are stored. In this case, you'll be pulling from the `imbalanced` train and dev directories.\n",
    "- `image_size`: The dimensions to which all images found will be resized. Since images come in all kinds of resolutions, you need to standardize their size. 150x150 is used here, but other values should work well too.\n",
    "- `batch_size`: Number of images the generator yields every time it is asked for a next batch. 32 is used here.\n",
    "- `label_mode`: How the labels are represented. Here `int` is used to indicate that labels will be 1D. This is done for compatibility with the loss and evaluation metrics used when compiling the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I_u6BMfBLqBp"
   },
   "outputs": [],
   "source": [
    "# Instantiate the Dataset object for the training set\n",
    "train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    os.path.join(DATA_DIR,'imbalanced/train'),\n",
    "    image_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    label_mode='int'\n",
    "    )\n",
    "\n",
    "# Instantiate the Dataset object for the dev set\n",
    "dev_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    os.path.join(DATA_DIR,'imbalanced/dev'),\n",
    "    image_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    label_mode='int'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X1UCTYy0ZK3k"
   },
   "source": [
    "Images are usually normalized to help with the model learning. You will use the [Rescaling](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Rescaling) preprocessing layer to transform the raw image pixels into the range `[0,1]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zWcuNa3pLls-"
   },
   "outputs": [],
   "source": [
    "# Define the layer to normalize the images\n",
    "rescale_layer = tf.keras.layers.Rescaling(1./255)\n",
    "\n",
    "# Apply the layer to the datasets\n",
    "train_dataset_scaled = train_dataset.map(lambda image, label: (rescale_layer(image), label))\n",
    "dev_dataset_scaled = dev_dataset.map(lambda image, label: (rescale_layer(image), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NsowgcmDAOv-"
   },
   "source": [
    "You will chain in a few more methods to the datasets. In addition to shuffling the training set, using the [cache()](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#cache) and [prefetch()](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#prefetch) will help speed up the training. You can read more about it [here](https://www.tensorflow.org/guide/data_performance#caching) and [here](https://www.tensorflow.org/guide/data_performance#prefetching)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8tMrXlmcOSUR"
   },
   "outputs": [],
   "source": [
    "SHUFFLE_BUFFER_SIZE = 1000\n",
    "PREFETCH_BUFFER_SIZE = tf.data.AUTOTUNE\n",
    "\n",
    "train_dataset_final = (train_dataset_scaled\n",
    "                       .cache()\n",
    "                       .shuffle(SHUFFLE_BUFFER_SIZE)\n",
    "                       .prefetch(buffer_size=PREFETCH_BUFFER_SIZE)\n",
    "                      )\n",
    "\n",
    "dev_dataset_final = (dev_dataset_scaled\n",
    "                            .cache()\n",
    "                            .prefetch(buffer_size=PREFETCH_BUFFER_SIZE)\n",
    "                           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UqXdzv-soUzj"
   },
   "source": [
    "##  Training a CNN with class imbalanced data\n",
    "\n",
    "Now you're ready to train the model. If you're using the recommended pretrained models, move to the next step to analyze the model performance. (If you intend to train the models yourself in an environment with a GPU, run the code cell below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UUhtEnsgxZ00"
   },
   "outputs": [],
   "source": [
    "# # Run only if you want to train the model yourself. This takes around 3 mins with GPU enabled on Colab.\n",
    "\n",
    "# # Create a model to use with the imbalanced dataset\n",
    "# imbalanced_model = lab_utils.create_model()\n",
    "\n",
    "# imbalanced_history = imbalanced_model.fit(\n",
    "#     train_dataset_final,\n",
    "#     epochs=10,\n",
    "#     validation_data=dev_dataset_final\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9kHwAYLvEhiQ"
   },
   "source": [
    "To analyze the model performance properly, it is important to track different metrics such as accuracy and loss during the training process. Plot the metrics and losses for each training epoch using the `lab_utils.plot_train_eval()` helper function. If you opted not to do the training yourself, the code below will load pre-generated files from one of our training sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BaNBP4py2YiV"
   },
   "outputs": [],
   "source": [
    "# This will succeed if the model was trained on Colab or an environment with GPU.\n",
    "try:\n",
    "    lab_utils.plot_train_eval(imbalanced_history)\n",
    "\n",
    "# If it fails, load pre-generated history and model files.\n",
    "except NameError:\n",
    "\n",
    "    # Load the history\n",
    "    with open('./histories/imbalanced_history.pkl', \"rb\") as pickle_file:\n",
    "        imbalanced_history = pickle.load(pickle_file)\n",
    "\n",
    "    # Load the pre-trained imbalanced model. This will be used in the next cell.\n",
    "    imbalanced_model = tf.keras.models.load_model('./models/imbalanced_model')\n",
    "\n",
    "    # Plot the train and dev accuracy and loss\n",
    "    lab_utils.plot_train_eval(imbalanced_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4mF4fltDFM6o"
   },
   "source": [
    "From these two plots, it's evident that the model is overfitting the training data. However, the validation accuracy is still pretty high. Maybe class imbalance is not such a big issue after all. Perhaps this is too good to be true.\n",
    "\n",
    "Let's dive a little deeper, and compute some additional metrics to explore if the class imbalance is hampering the model to perform well. In particular, let's compare: the [accuracy score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html),  the [balanced accuracy score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html), and the [confusion matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html).  Information on the accuracy scores calculations is provided in the [sklearn](https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics) documentation. You can also review the confusion matrix [here](https://en.wikipedia.org/wiki/Confusion_matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yJEg83EIW_jm"
   },
   "outputs": [],
   "source": [
    "# Get the true labels from the dev set\n",
    "y_true = dev_dataset_final.map(lambda image, label: label).unbatch()\n",
    "y_true = list(y_true)\n",
    "\n",
    "# Use the model to predict. This will take about a minute\n",
    "predictions_imbalanced = imbalanced_model.predict(dev_dataset_final)\n",
    "\n",
    "# Get the argmax (since softmax is being used)\n",
    "y_pred_imbalanced = np.argmax(predictions_imbalanced, axis=1)\n",
    "\n",
    "# Print accuracy score\n",
    "print(f\"Accuracy Score: {accuracy_score(y_true, y_pred_imbalanced)}\")\n",
    "\n",
    "# Print balanced accuracy score\n",
    "print(f\"Balanced Accuracy Score: {balanced_accuracy_score(y_true, y_pred_imbalanced)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cXQQR9D8HVUh"
   },
   "source": [
    "Comparing the `accuracy` and `balanced accuracy` metrics, the class imbalance starts to become apparent. Now, compute the `confusion matrix` of the predictions. Notice that the class imbalance is also present in the dev set, so the confusion matrix will show an overwhelming majority of predictions for cats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zZqpe9uLN2k0"
   },
   "outputs": [],
   "source": [
    "imbalanced_cm = confusion_matrix(y_true, y_pred_imbalanced)\n",
    "\n",
    "ConfusionMatrixDisplay(imbalanced_cm, display_labels=['birds', 'cats', 'dogs']).plot(values_format=\"d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QxLdL5eYZK3m"
   },
   "source": [
    "If you show the misclassified images per class, you'll notice that the model expectedly does well for cats but not so much for birds and dogs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nu3xXDhYAnqL"
   },
   "outputs": [],
   "source": [
    "misclassified_birds = (imbalanced_cm[0, 1] + imbalanced_cm[0, 2])/np.sum(imbalanced_cm, axis=1)[0]\n",
    "misclassified_cats = (imbalanced_cm[1, 0] + imbalanced_cm[1, 2])/np.sum(imbalanced_cm, axis=1)[1]\n",
    "misclassified_dogs = (imbalanced_cm[2, 0] + imbalanced_cm[2, 1])/np.sum(imbalanced_cm, axis=1)[2]\n",
    "\n",
    "print(f\"Proportion of misclassified birds: {misclassified_birds*100:.2f}%\")\n",
    "print(f\"Proportion of misclassified cats: {misclassified_cats*100:.2f}%\")\n",
    "print(f\"Proportion of misclassified dogs: {misclassified_dogs*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e3tpDKCsT564"
   },
   "source": [
    "Class imbalance is a real problem. If it's not detected early on, it can give the wrong impression that your model is performing better than it actually is. For this reason, it is important to rely on several metrics that do a better job at capturing these kinds of issues. **In this case, the overall `accuracy` metric is misleading** because the good results for the dominant class (i.e. cats) hide the poor results in the other two.\n",
    "\n",
    "To prove this point further, consider a model that only predicts cats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yv65fC5NK5sV"
   },
   "outputs": [],
   "source": [
    "# Predict cat for all images\n",
    "all_cats = np.ones(len(y_true))\n",
    "\n",
    "# Print accuracy score\n",
    "print(f\"Accuracy Score: {accuracy_score(y_true, all_cats)}\")\n",
    "\n",
    "# Print balanced accuracy score\n",
    "print(f\"Balanced Accuracy Score: {balanced_accuracy_score(y_true, all_cats)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g_Gp6mYcIQlW"
   },
   "source": [
    "If you only look at the `accuracy` metric, the model seems to be working fairly well, since the majority class is the same that the model always predicts.\n",
    "\n",
    "To address class imbalance, you can simply gather more data or use other techniques. For instance, [SMOTE](https://arxiv.org/pdf/1106.1813.pdf) oversamples the minority classes by creating synthetic data. However, these techniques are outside the scope of this lab.\n",
    "\n",
    "Before moving forward, you can free up some memory by running the next cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "weNpR6EhZK3m"
   },
   "outputs": [],
   "source": [
    "# Delete unused variables\n",
    "del train_dataset\n",
    "del dev_dataset\n",
    "del train_dataset_scaled\n",
    "del dev_dataset_scaled\n",
    "del train_dataset_final\n",
    "del dev_dataset_final\n",
    "del imbalanced_history\n",
    "del imbalanced_model\n",
    "del y_true\n",
    "del predictions_imbalanced\n",
    "del y_pred_imbalanced\n",
    "del all_cats\n",
    "\n",
    "# Call the Python garbage collector to free memory\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5vJRVjlQvK-"
   },
   "source": [
    "# Training with the complete dataset\n",
    "\n",
    "For the time being and following the narrative, assume that a colleague of yours was careful enough to save a backup of the complete dataset in her cloud storage. Now you can try training without the class imbalance issue. What a relief!\n",
    "\n",
    "You will now train the model with the balanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yJkIBdLtTvT5"
   },
   "outputs": [],
   "source": [
    "# Instantiate the Dataset object for the training set\n",
    "train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    os.path.join(DATA_DIR,'train'),\n",
    "    image_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    label_mode='int'\n",
    "    )\n",
    "\n",
    "# Instantiate the Dataset object for the dev set\n",
    "dev_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    os.path.join(DATA_DIR,'dev'),\n",
    "    image_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    label_mode='int'\n",
    "    )\n",
    "\n",
    "# Define the layer to normalize the images\n",
    "rescale_layer = tf.keras.layers.Rescaling(1./255)\n",
    "\n",
    "# Apply the layer to the datasets\n",
    "train_dataset_scaled = train_dataset.map(lambda image, label: (rescale_layer(image), label))\n",
    "dev_dataset_scaled = dev_dataset.map(lambda image, label: (rescale_layer(image), label))\n",
    "\n",
    "train_dataset_final = train_dataset_scaled.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "dev_dataset_final = dev_dataset_scaled.cache().prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jAXOFxLgZK3n"
   },
   "source": [
    "<details>\n",
    "  <summary><font size=\"3\" color=\"blue\"><b>Click here if you are training the model yourself and the Colab runtime crashed</b></font></summary>\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "The Colab RAM may not be sufficient to run all the exercises here in one go. If the runtime crashes while training, you can recover by:\n",
    "\n",
    "* Going to the top of this lab\n",
    "* Run the cells in the `Lab Setup` section,  particularly the imports and global variable definitions.\n",
    "* **There is no need to run the cells in the `Preparing the Data` and `Training a CNN with class imbalanced data` sections.\n",
    "* Scroll back to this section (i.e. `Training with the complete dataset`) and run the cells again to start training.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NcOh1NVtm5Dg"
   },
   "outputs": [],
   "source": [
    "# # Run only if you want to train the model yourself. This takes around 5 minutes on Colab.\n",
    "\n",
    "# # Create a model to use with the balanced dataset\n",
    "# balanced_model = lab_utils.create_model()\n",
    "\n",
    "# balanced_history = balanced_model.fit(\n",
    "#     train_dataset_final,\n",
    "#     epochs=10,\n",
    "#     validation_data=dev_dataset_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WC7-I1ylr-_n"
   },
   "outputs": [],
   "source": [
    "# This will succeed if the model was trained on Colab or an environment with GPU.\n",
    "try:\n",
    "    balanced_history\n",
    "\n",
    "# If it fails, load pre-generated history and model files.\n",
    "except NameError:\n",
    "\n",
    "    # Load the history\n",
    "    with open('./histories/balanced_history.pkl', \"rb\") as pickle_file:\n",
    "        balanced_history = pickle.load(pickle_file)\n",
    "\n",
    "    # Load the pre-trained imbalanced model. This will be used in the next cell.\n",
    "    balanced_model = tf.keras.models.load_model('./models/balanced_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7LZUa9RVvyX"
   },
   "source": [
    "Let's check how the `accuracy` vs `balanced accuracy` comparison looks like now:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IwQCifZJZK3n"
   },
   "source": [
    "<details>\n",
    "  <summary><font size=\"3\" color=\"blue\"><b>Click here if the cell below restarts the kernel</b></font></summary>\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "Sometimes, freeing the RAM gets delayed so running the cell below might crash after the previous exercises. If that happens, you can recover by:\n",
    "\n",
    "* Going to the top of this lab\n",
    "* Run the cells in the `Lab Setup` section,  particularly the imports and global variable definitions.\n",
    "* **There is no need to run the cells in the `Preparing the Data` and `Training a CNN with class imbalanced data` sections.\n",
    "* Scroll back to this section (i.e. `Training with the complete dataset`) and run the cells again to start the model evaluation.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wfLgvWRfKuTQ"
   },
   "outputs": [],
   "source": [
    "# Get the true labels\n",
    "y_true = dev_dataset_final.map(lambda image, label: label).unbatch()\n",
    "y_true = list(y_true)\n",
    "\n",
    "# Use the model to predict (will take a couple of minutes)\n",
    "predictions_balanced = balanced_model.predict(dev_dataset_final)\n",
    "\n",
    "# Get the argmax (since softmax is being used)\n",
    "y_pred_balanced = np.argmax(predictions_balanced, axis=1)\n",
    "\n",
    "# Print accuracy score\n",
    "print(f\"Accuracy Score: {accuracy_score(y_true, y_pred_balanced)}\")\n",
    "\n",
    "# Print balanced accuracy score\n",
    "print(f\"Balanced Accuracy Score: {balanced_accuracy_score(y_true, y_pred_balanced)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Mpnmv5YKyeD"
   },
   "outputs": [],
   "source": [
    "balanced_cm = confusion_matrix(y_true, y_pred_balanced)\n",
    "ConfusionMatrixDisplay(balanced_cm, display_labels=['birds', 'cats', 'dogs']).plot(values_format=\"d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dp7QCgZ0Wuf3"
   },
   "source": [
    "Both accuracy-based metrics are very similar now. The confusion matrix also looks way better than before. This result suggests that class imbalance has been successfully mitigated by adding more data to the previously undersampled classes.\n",
    "\n",
    "Now that you know that you can trust the `accuracy` metric, plot the training history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6pr2VmKtJpet"
   },
   "outputs": [],
   "source": [
    "lab_utils.plot_train_eval(balanced_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YCH1hTj7JvHu"
   },
   "source": [
    "This looks better than the earlier graphs for the imbalanced case. However, overfitting is still present.\n",
    "\n",
    "Can you think of ways to address this issue? If you are familiar with CNNs, you might think of adding [Dropout](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout) layers. This intuition is correct, but for now, you decide to stick with the same model and only change the data to see if it is possible to mitigate overfitting this way.\n",
    "\n",
    "Keeping the model constant, one other possible solution to address overfitting is to apply data augmentation. Your whole team agrees this is the way to go so you decide to try this next.\n",
    "\n",
    "Before moving forward, you can free up some memory by running the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GAL-7N4TZK3o"
   },
   "outputs": [],
   "source": [
    "# Delete unused variables\n",
    "del train_dataset\n",
    "del dev_dataset\n",
    "del train_dataset_scaled\n",
    "del dev_dataset_scaled\n",
    "del train_dataset_final\n",
    "del dev_dataset_final\n",
    "del balanced_history\n",
    "del balanced_model\n",
    "del y_true\n",
    "del predictions_balanced\n",
    "del y_pred_balanced\n",
    "\n",
    "# Call the Python garbage collector to free memory\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VdlVWEZuX4ii"
   },
   "source": [
    "# Training with Data Augmentation\n",
    "\n",
    "Augmenting images is a technique where you create new versions of existing images in the dataset by applying geometric transformations. These transformations can vary from zooming in and out, rotating, or even flipping the images. By performing these transformations, you get a training dataset that exposes the model to a wider variety of images. This technique helps to further explore the feature space and hence reduce the chance of overfitting.\n",
    "\n",
    "It is also a very natural idea since doing slight (or sometimes not so slight) changes to an image will result in an equally valid image. A cat sitting in an awkward position is still a cat, right?\n",
    "\n",
    "You can do data augmentation in Keras by using [image augmentation layers](https://keras.io/api/layers/preprocessing_layers/image_augmentation/). These are only active during training and will automatically be bypassed during model prediction. The cell below creates a data augmentation model that flips, rotates, translates, and zooms the images randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZuPY39HHTvT8"
   },
   "outputs": [],
   "source": [
    "# Define fill mode.\n",
    "FILL_MODE = 'nearest'\n",
    "\n",
    "# Create the augmentation model.\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "\n",
    "        # Specify the input shape on the first layer.\n",
    "        tf.keras.layers.RandomFlip(\"horizontal\", input_shape=(150,150,3)),\n",
    "        tf.keras.layers.RandomRotation(0.2, fill_mode=FILL_MODE),\n",
    "        tf.keras.layers.RandomTranslation(0.2,0.2, fill_mode=FILL_MODE),\n",
    "        tf.keras.layers.RandomZoom(0.2, fill_mode=FILL_MODE)\n",
    "\n",
    "        ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9osuJVkMZK3o"
   },
   "source": [
    "You can pass in some images to the `lab_utils.demo_augmentation()` function and see how they are transformed by these layers. First, get a sample batch from the training dataset and put the images in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IbiKcjhaZK3p"
   },
   "outputs": [],
   "source": [
    "# Instantiate the Dataset object for the training set\n",
    "train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    os.path.join(DATA_DIR,'train'),\n",
    "    image_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    label_mode='int'\n",
    "    )\n",
    "\n",
    "# Instantiate the Dataset object for the dev set\n",
    "dev_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    os.path.join(DATA_DIR,'dev'),\n",
    "    image_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    label_mode='int'\n",
    "    )\n",
    "\n",
    "# Define the layer to normalize the images\n",
    "rescale_layer = tf.keras.layers.Rescaling(1./255)\n",
    "\n",
    "# Apply the layer to the datasets\n",
    "train_dataset_scaled = train_dataset.map(lambda image, label: (rescale_layer(image), label))\n",
    "dev_dataset_scaled = dev_dataset.map(lambda image, label: (rescale_layer(image), label))\n",
    "\n",
    "train_dataset_final = train_dataset_scaled.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "dev_dataset_final = dev_dataset_scaled.cache().prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OadxyqS6TvT8"
   },
   "outputs": [],
   "source": [
    "# Get a batch of images\n",
    "sample_batch = list(train_dataset.take(1))[0][0]\n",
    "print(f'images per batch: {len(sample_batch)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "98PbjctOZK3p"
   },
   "source": [
    "Then pass some images from this list to the utility function. The cell below modifies the original images four times and previews the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rEvqwEc1TvT8"
   },
   "outputs": [],
   "source": [
    "NUM_AUG = 4\n",
    "\n",
    "# Apply the transformations to the first 4 images\n",
    "lab_utils.demo_augmentation(sample_batch[0], data_augmentation, NUM_AUG)\n",
    "lab_utils.demo_augmentation(sample_batch[1], data_augmentation, NUM_AUG)\n",
    "lab_utils.demo_augmentation(sample_batch[2], data_augmentation, NUM_AUG)\n",
    "lab_utils.demo_augmentation(sample_batch[3], data_augmentation, NUM_AUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KaKZ624jBlt6"
   },
   "source": [
    "Feel free to modify the parameters in the data augmentation model and see how it transforms the original images. Now that you know what it is doing to the training images, you can move onto training. You will prepend the data augmentation model to the base model as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g7RAqkSRC98K"
   },
   "outputs": [],
   "source": [
    "# Instantiate the base model\n",
    "base_model = lab_utils.create_model()\n",
    "\n",
    "# Prepend the data augmentation layers to the base model\n",
    "augmented_model = tf.keras.models.Sequential([\n",
    "    data_augmentation,\n",
    "    base_model\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "augmented_model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['sparse_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6qRwmk_WZK3p"
   },
   "source": [
    "<details>\n",
    "  <summary><font size=\"3\" color=\"blue\"><b>Click here if you are training the model yourself and the Colab runtime crashed</b></font></summary>\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "The Colab RAM may not be sufficient to run all the exercises here in one go. If the runtime crashes while training, you can recover by:\n",
    "\n",
    "* Going to the top of this lab\n",
    "* Run the cells in the `Lab Setup` section,  particularly the imports and global variable definitions.\n",
    "* **There is no need to run the cells in the `Preparing the Data`, `Training a CNN with class imbalanced data`, and `Training with a complete dataset` sections.**\n",
    "* Scroll back to this section (i.e. `Training with Data Augmentation`) and run the cells again to start training.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7aSV4CyGHRz-"
   },
   "outputs": [],
   "source": [
    "# # # Run only if you want to train the model yourself (this takes around 20 mins with GPU enabled)\n",
    "\n",
    "# augmented_history = augmented_model.fit(\n",
    "#     train_dataset_final,\n",
    "#     epochs=40,\n",
    "#     validation_data=dev_dataset_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d0hoorf7brwZ"
   },
   "source": [
    "Since you know that class imbalance is no longer an issue, you can skip checking for more in-depth metrics and instead plot the training history right away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8EYc1oXmHjE2"
   },
   "outputs": [],
   "source": [
    "# This will succeed if the model was trained on Colab or an environment with GPU.\n",
    "try:\n",
    "    lab_utils.plot_train_eval(augmented_history)\n",
    "\n",
    "# If it fails, load pre-generated history.\n",
    "except NameError:\n",
    "\n",
    "    # Load the history\n",
    "    with open('./histories/augmented_history.pkl', \"rb\") as pickle_file:\n",
    "        augmented_history = pickle.load(pickle_file)\n",
    "\n",
    "    # Plot the train and dev accuracy and loss\n",
    "    lab_utils.plot_train_eval(augmented_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nBy1VcxacPEx"
   },
   "source": [
    "Now, the validation accuracy follows the training accuracy more closely. This finding indicates that **the model is no longer overfitting**, which is quite a remarkable result achieved by just augmenting the existing images.\n",
    "\n",
    "Another point worth mentioning is that this model achieves a slightly lower validation accuracy compared to the model without data augmentation. The model needs more epochs to train because it's dealing with a lot more variations in the dataset (due to the augmentations).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dOA93ENHczla"
   },
   "source": [
    "## Wrapping it up\n",
    "\n",
    "**Congratulations on finishing this ungraded lab!**\n",
    "\n",
    "It is quite amazing to see how data alone can impact machine learning models. Hopefully, this lab helped you better understand the importance of data.\n",
    "\n",
    "In particular, you figured out ways to diagnose the effects of class imbalance and looked at specific metrics to spot this problem. Adding more data is a simple way to overcome class imbalance. However, this solution is not always feasible in a real life scenario.\n",
    "\n",
    "In the final section, you applied multiple geometric transformations to the images in the training dataset to generate additional data. The goal was to use data augmentation to reduce overfitting. Changing the network architecture is also an alternative method to address this problem. In practice, it is a good idea to implement both techniques for better results.\n",
    "\n",
    "\n",
    "**Keep it up!**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
