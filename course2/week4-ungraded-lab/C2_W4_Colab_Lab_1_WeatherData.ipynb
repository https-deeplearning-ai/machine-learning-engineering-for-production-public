{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3IrnqNnWO04A"
   },
   "source": [
    "# Ungraded Lab: Feature Engineering with Weather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mtQWmTSsO_-Q"
   },
   "source": [
    "In this 1st exercise on feature engineering with time series data, you will practice data transformation with the [`weather dataset`](https://www.bgc-jena.mpg.de/wetter/) recorded by the [Max Planck Institute for Biogeochemistry](https://www.bgc-jena.mpg.de/). \n",
    "\n",
    "This dataset contains 14 different features such as air temperature, atmospheric pressure, and humidity. \n",
    "\n",
    "These were collected every 10 minutes, beginning in 2003. For this lab, you will use only the data collected between 2009 and 2016. This section of the dataset was prepared by François Chollet for his book *Deep Learning with Python*.\n",
    "\n",
    "The table below shows the column names, their value formats, and their description.\n",
    "\n",
    "Index| Features      |Format             |Description\n",
    "-----|---------------|-------------------|-----------------------\n",
    "1    |Date Time      |01.01.2009 00:10:00|Date-time reference\n",
    "2    |p (mbar)       |996.52             |The pascal SI derived unit of pressure used to quantify internal pressure. Meteorological reports typically state atmospheric pressure in millibars.\n",
    "3    |T (degC)       |-8.02              |Temperature in Celsius\n",
    "4    |Tpot (K)       |265.4              |Temperature in Kelvin\n",
    "5    |Tdew (degC)    |-8.9               |Temperature in Celsius relative to humidity. Dew Point is a measure of the absolute amount of water in the air, the DP is the temperature at which the air cannot hold all the moisture in it and water condenses.\n",
    "6    |rh (%)         |93.3               |Relative Humidity is a measure of how saturated the air is with water vapor, the %RH determines the amount of water contained within collection objects.\n",
    "7    |VPmax (mbar)   |3.33               |Saturation vapor pressure\n",
    "8    |VPact (mbar)   |3.11               |Vapor pressure\n",
    "9    |VPdef (mbar)   |0.22               |Vapor pressure deficit\n",
    "10   |sh (g/kg)      |1.94               |Specific humidity\n",
    "11   |H2OC (mmol/mol)|3.12               |Water vapor concentration\n",
    "12   |rho (g/m ** 3) |1307.75            |Airtight\n",
    "13   |wv (m/s)       |1.03               |Wind speed\n",
    "14   |max. wv (m/s)  |1.75               |Maximum wind speed\n",
    "15   |wd (deg)       |152.3              |Wind direction in degrees\n",
    "\n",
    "You will perform data preprocessing so that the features can be used to train an LSTM using TensorFlow and Keras downstream. You will not be asked to train a model as the focus is on feature preprocessing. However in later courses of this specialization, you will get to learn more about time series forecasting, at which point you can retreive your saved tfrecords and train a neural net model for weather forecasting.\n",
    "\n",
    "Upon completion, you will have\n",
    "\n",
    "* Explored and visualized the weather time series dataset and declared its schema\n",
    "* Transformed the data for modeling using TF Transform\n",
    "* Prepared Training Dataset Windows from `TFTransformOutput`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iyHZtPotQhL0"
   },
   "source": [
    "## Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7yOlLLTMHayh"
   },
   "outputs": [],
   "source": [
    "# Install TFX and tensorflow_addons\n",
    "!pip install -v tfx==0.24.1\n",
    "!pip install tensorflow_addons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZRzTwQUtrbPj"
   },
   "source": [
    "*Note: In Google Colab, you need to restart the runtime at this point to finalize updating the packages you just installed. **Please do not proceed to the next section without restarting.** You can also ignore the errors about version incompatibility of some of the bundled packages because we won't be using those in this notebook.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KUzP274BxLjx"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RjkRoC4BHWW5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "import tempfile\n",
    "import urllib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import absl\n",
    "import tensorflow as tf\n",
    "tf.get_logger().propagate = False\n",
    "pp = pprint.PrettyPrinter()\n",
    "\n",
    "import tfx\n",
    "from tfx.components import CsvExampleGen\n",
    "from tfx.components import ExampleValidator\n",
    "from tfx.components import SchemaGen\n",
    "from tfx.components import StatisticsGen\n",
    "from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n",
    "from tfx.types import Channel\n",
    "from tfx.utils.dsl_utils import external_input\n",
    "from tfx.components.transform.component import Transform\n",
    "\n",
    "from google.protobuf.json_format import MessageToDict\n",
    "\n",
    "print(f'TFX version: {tfx.__version__}')\n",
    "print(f'Tensorflow version: {tf.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ro4UcYpAr7kQ"
   },
   "source": [
    "## Download the dataset\n",
    "\n",
    "Let's then setup the dataset in your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HdhqHQieHkmj"
   },
   "outputs": [],
   "source": [
    "# Dataset directory\n",
    "!mkdir -p data/climate\n",
    "\n",
    "# TFX pipeline files directory\n",
    "!mkdir pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xt4_VsSIHWW6"
   },
   "outputs": [],
   "source": [
    "# location of the pipeline metadata store\n",
    "_pipeline_root = './pipeline/'\n",
    "\n",
    "# directory of the raw data files\n",
    "_data_root = './data/climate'\n",
    "\n",
    "# path to the raw training data\n",
    "_data_filepath = os.path.join(_data_root, 'jena_climate_2009_2016.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_3-31XlkWoVv"
   },
   "outputs": [],
   "source": [
    "# Download the dataset to the data directory\n",
    "!wget https://raw.githubusercontent.com/https-deeplearning-ai/MLEP-public/main/course2/week4-ungraded-lab/data/jena_climate_2009_2016.csv -P {_data_root}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dy6nZ4c9uxsB"
   },
   "source": [
    "Let's preview the first few rows. You will notice that almost all the features are numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZMXsrc5_HWW6"
   },
   "outputs": [],
   "source": [
    "# Preview the dataset\n",
    "!head {_data_filepath}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PPbYBDMP8pR-"
   },
   "source": [
    "You will also notice that there are quotes in the column names. It is up to you if you should keep it but for this exercise, it would be better to remove it so we don't have to factor it in when we're declaring the keys in the Transform module. Let's do that below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xGaAcsb4hYut"
   },
   "outputs": [],
   "source": [
    "# Load the CSV into a dataframe\n",
    "df = pd.read_csv(f'{_data_filepath}')\n",
    "\n",
    "# Remove the quotes in the column names\n",
    "df.columns=df.columns.str.replace('\"','')\n",
    "\n",
    "# Save the changes\n",
    "df.to_csv(f'{_data_filepath}', index=False)\n",
    "\n",
    "# See the result\n",
    "!head {_data_filepath}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UDjP3xTVJyzK"
   },
   "source": [
    "## Data Pipeline\n",
    "\n",
    "Since you already know how to run TFX pipelines, we will just quickly go over the first few components. The main difference we need to discuss will be in the Transform part where you need to transform certain columns into usable signals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K0f6X7Zgvd_x"
   },
   "source": [
    "## Create the InteractiveContext\n",
    "\n",
    "As usual, you need to initialize the `InteractiveContext` so you can run the components in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LULK11m7HWW6"
   },
   "outputs": [],
   "source": [
    "# Initialize the InteractiveContext\n",
    "context = InteractiveContext(pipeline_root=_pipeline_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bpbMn8cHv1hf"
   },
   "source": [
    "## ExampleGen\n",
    "\n",
    "You will then ingest the data from the dataframe you defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7hJecU3UHWW7"
   },
   "outputs": [],
   "source": [
    "# Run CSV ExampleGen\n",
    "example_gen = CsvExampleGen(input_base=_data_root)  \n",
    "context.run(example_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q7dkHuh8wI7k"
   },
   "source": [
    "## StatisticsGen\n",
    "\n",
    "Next, you will generate the statistics that will be used by the next components. Feel free to also explore it when you run `context.show()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u97zWVKPHWW8"
   },
   "outputs": [],
   "source": [
    "# Instantiate StatisticsGen with the ExampleGen ingested dataset\n",
    "statistics_gen = StatisticsGen(\n",
    "    examples=example_gen.outputs['examples'])\n",
    "\n",
    "# Execute the component\n",
    "context.run(statistics_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cow3IXRmHWW8"
   },
   "outputs": [],
   "source": [
    "# Show the output statistics\n",
    "context.show(statistics_gen.outputs['statistics'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Cm93aA7zt6R"
   },
   "source": [
    "From the charts above, you might notice that the minimum value for `wv (m/s)` is `-9999`. Those are pretty intense winds and based on the other data points, this just looks like a faulty measurement. You can also use the utilities below to inspect the dataset. You will notice this `-9999` value as an outlier in the `wv (m/s)` plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BTjN4j1z5Hj2"
   },
   "outputs": [],
   "source": [
    "# Visualization Utilities\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Color Palette\n",
    "colors = [\n",
    "    \"blue\",\n",
    "    \"orange\",\n",
    "    \"green\",\n",
    "    \"red\",\n",
    "    \"purple\",\n",
    "    \"brown\",\n",
    "    \"pink\",\n",
    "    \"gray\",\n",
    "    \"olive\",\n",
    "    \"cyan\",\n",
    "]\n",
    "\n",
    "# Dataset Columns\n",
    "feature_keys = [\n",
    "    \"p (mbar)\",\n",
    "    \"T (degC)\",\n",
    "    \"Tpot (K)\",\n",
    "    \"Tdew (degC)\", \n",
    "    \"rh (%)\", \n",
    "    \"VPmax (mbar)\", \n",
    "    \"VPact (mbar)\", \n",
    "    \"VPdef (mbar)\", \n",
    "    \"sh (g/kg)\",\n",
    "    \"H2OC (mmol/mol)\",\n",
    "    \"rho (g/m**3)\",\n",
    "    \"wv (m/s)\",\n",
    "    \"max. wv (m/s)\",\n",
    "    \"wd (deg)\",\n",
    "]\n",
    "\n",
    "# Plots each column as a time series\n",
    "def visualize_plots(dataset, columns):\n",
    "    features = dataset[columns]\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=len(columns)//2 + len(columns)%2, ncols=2, figsize=(15, 20), dpi=80, facecolor=\"w\", edgecolor=\"k\"\n",
    "    )\n",
    "    for i, col in enumerate(columns):\n",
    "        c = colors[i % (len(colors))]\n",
    "        t_data = dataset[col]\n",
    "        t_data.index = dataset.index\n",
    "        t_data.head()\n",
    "        ax = t_data.plot(\n",
    "            ax=axes[i // 2, i % 2],\n",
    "            color=c,\n",
    "            title=\"{}\".format(col),\n",
    "            rot=25,\n",
    "        )\n",
    "    ax.legend([col])\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9x373IOb305x"
   },
   "outputs": [],
   "source": [
    "# Visualize the dataset\n",
    "visualize_plots(df, feature_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XKJirin_380u"
   },
   "source": [
    "In real projects, you will need to clean this up. If you want, you can drop those rows from the CSV and ingest the data again through ExampleGen. For this exercise however, we will just proceed to the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pzU4peVEwNHp"
   },
   "source": [
    "## SchemaGen\n",
    "\n",
    "You will now infer the schema based on the statistics artifact. For the version of TFX you are using, you will have to explicitly set `infer_feature_shape=True` so the downstream TFX components (e.g. Transform) will parse input as a `Tensor` and not `SparseTensor`. If not set, you will have compatibility issues later when you run the transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hS4QtClPHWW9"
   },
   "outputs": [],
   "source": [
    "# Instantiate SchemaGen with the StatisticsGen ingested dataset\n",
    "schema_gen = SchemaGen(\n",
    "    statistics=statistics_gen.outputs['statistics'], \n",
    "    infer_feature_shape=True\n",
    "    )\n",
    "\n",
    "# Run the component\n",
    "context.run(schema_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_otWswaxHWW9"
   },
   "outputs": [],
   "source": [
    "# Visualize the schema\n",
    "context.show(schema_gen.outputs['schema'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fz1MtS7vi99h"
   },
   "source": [
    "Again, for the purpose of this exercise, we will accept this initial version of the schema. In actual projects though, you may want to restrict the domain (e.g. numeric range) so you can avoid invalid data from going into your training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-fuQn9N9wVR2"
   },
   "source": [
    "## ExampleValidator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eEpiqHyjjhLL"
   },
   "source": [
    "Let's check for anomalies to ensure our dataset conforms to the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9uqeNI2pHWW9"
   },
   "outputs": [],
   "source": [
    "# Instantiate ExampleValidator with the StatisticsGen and SchemaGen ingested data\n",
    "example_validator = ExampleValidator(\n",
    "    statistics=statistics_gen.outputs['statistics'],\n",
    "    schema=schema_gen.outputs['schema'])\n",
    "\n",
    "# Run the component.\n",
    "context.run(example_validator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D7W4sSk1HWW-"
   },
   "outputs": [],
   "source": [
    "# Visualize the results\n",
    "context.show(example_validator.outputs['anomalies'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PGVR9oI9wYDl"
   },
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t0yT60dMIttI"
   },
   "source": [
    "Now you will be doing feature engineering. There are several things to note before doing the transformation:\n",
    "\n",
    "### Correlated features\n",
    "\n",
    "You may want to drop redundant features to reduce the complexity of your model. Let's see what features are highly correlated with each other by plotting the correlation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m_A5v20M6w7J"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def show_correlation_heatmap(dataframe):\n",
    "    plt.figure(figsize=(20,20))\n",
    "    cor = dataframe.corr()\n",
    "    sns.heatmap(cor, annot=True, cmap=plt.cm.PuBu)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rYMNjP0362us"
   },
   "outputs": [],
   "source": [
    "show_correlation_heatmap(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5yqKqnN6674y"
   },
   "source": [
    "You can observe that `Tpot (K)`, `Tdew (degC)`, `VPmax(mbar)`, `Vpact(mbar)`, `VPdef (mbar)`, `sh(g/kg)` and `H2OC` are highly positively correlated to the target `T (degC)`. Likewise, `rho` is highly negatively correlated to the target. \n",
    "\n",
    "In the features that are positively correlated, you can see that `VPmax (mbar)` is highly correlated to some features like `Vpact (mbar)`, `Tdew (degC)` and `Tpot (K)`. Hence, for the sake of this exercise you can drop these features and retain `VPmax (mbar)`. This step will be done in the Transform module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bgzfs_qY6TsC"
   },
   "source": [
    "#### Distribution of Wind Data\n",
    "\n",
    "The last column of the data, `wd (deg)`, gives the wind direction in units of degrees. However, angles in this current format do not make good model inputs. 360° and 0° should be close to each other and wrap around smoothly. Direction shouldn't matter if the wind is not blowing. This will be easier for the model to interpret if you convert the wind direction and velocity columns to a wind vector. Observe how sine and cosine are used to generate wind vector features (`Wx` and `Wy`) in the Transform module below.\n",
    "\n",
    "#### Date Time Feature\n",
    "\n",
    "Similarly, the `Date Time` column is very useful but not in the current string format. First, you need to convert it to to seconds. Being weather data, it has clear daily and yearly periodicity, and you need to take that into account.\n",
    "\n",
    "A simple approach to convert it to a usable signal is to again use sine and cosine to convert the time to clear \"Time of day\" (`Day sin`, `Day cos`) and \"Time of year\" (`Year sin`, `Year cos`) signals. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tDvmu-4V9s_J"
   },
   "source": [
    "### Transform Module\n",
    "\n",
    "With the three points above considered, you can now build the transform module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_cNHvRSPHWW-"
   },
   "outputs": [],
   "source": [
    "# Set the constants module filename\n",
    "_weather_constants_module_file = 'weather_constants.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gxzpp6TqHWW_"
   },
   "outputs": [],
   "source": [
    "%%writefile {_weather_constants_module_file}\n",
    "\n",
    "# Selected numeric features to transform\n",
    "SELECTED_NUMERIC_FEATURES = ['T (degC)', 'VPmax (mbar)', 'VPdef (mbar)', 'sh (g/kg)','rho (g/m**3)']\n",
    "\n",
    "# Utility function for renaming the feature\n",
    "def transformed_name(key):\n",
    "    return key + '_xf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7nBKJOKaHWW_"
   },
   "outputs": [],
   "source": [
    "# Set the transform module filename\n",
    "_weather_transform_module_file = 'weather_transform.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "16F5e6QxHWW_"
   },
   "outputs": [],
   "source": [
    "%%writefile {_weather_transform_module_file}\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "\n",
    "import weather_constants\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "import math as m\n",
    "\n",
    "# Features to filter out\n",
    "FEATURES_TO_REMOVE = [\"Tpot (K)\", \"Tdew (degC)\",\"VPact (mbar)\" , \"H2OC (mmol/mol)\", \"max. wv (m/s)\"]\n",
    "\n",
    "# Unpack the contents of the constants module\n",
    "_SELECTED_NUMERIC_FEATURE_KEYS = weather_constants.SELECTED_NUMERIC_FEATURES\n",
    "_transformed_name = weather_constants.transformed_name\n",
    "\n",
    "# Define the transformations\n",
    "def preprocessing_fn(inputs):\n",
    "    \"\"\"Preprocess input columns into transformed columns.\"\"\"\n",
    "\n",
    "    outputs = inputs.copy()\n",
    "\n",
    "    # Filter redundant features\n",
    "    for key in FEATURES_TO_REMOVE:\n",
    "        del outputs[key]\n",
    "\n",
    "    # Convert degrees to radians\n",
    "    pi = tf.constant(m.pi)\n",
    "    wd_rad = inputs['wd (deg)'] * pi / 180.0\n",
    "\n",
    "    # Calculate the wind x and y components.\n",
    "    outputs['Wx'] = inputs['wv (m/s)'] * tf.math.cos(wd_rad)\n",
    "    outputs['Wy'] = inputs['wv (m/s)'] * tf.math.sin(wd_rad)\n",
    "\n",
    "    # Delete `wv (m/s)` after getting the wind vector\n",
    "    del outputs['wv (m/s)']\n",
    "\n",
    "    # Get day and year in seconds\n",
    "    day = tf.cast(24*60*60, tf.float32)\n",
    "    year = tf.cast((365.2425)*day, tf.float32)\n",
    "\n",
    "    # Convert `Date Time` column into timestamps in seconds (using tfa helper function)\n",
    "    timestamp_s = tfa.text.parse_time(outputs['Date Time'], time_format='%d.%m.%Y %H:%M:%S', output_unit='SECOND')\n",
    "    timestamp_s = tf.cast(timestamp_s, tf.float32)\n",
    "    \n",
    "    # Convert timestamps into periodic signals\n",
    "    outputs['Day sin'] = tf.math.sin(timestamp_s * (2 * pi / day))\n",
    "    outputs['Day cos'] = tf.math.cos(timestamp_s * (2 * pi / day))\n",
    "    outputs['Year sin'] = tf.math.sin(timestamp_s * (2 * pi / year))\n",
    "    outputs['Year cos'] = tf.math.cos(timestamp_s * (2 * pi / year))\n",
    "\n",
    "    # Delete unneeded columns\n",
    "    del outputs['Date Time']\n",
    "    del outputs['wd (deg)']\n",
    "\n",
    "    # Final feature list\n",
    "    FINAL_FEATURE_LIST =  [\"p (mbar)\",\n",
    "    \"T (degC)\",\n",
    "    \"rh (%)\", \n",
    "    \"VPmax (mbar)\", \n",
    "    \"VPdef (mbar)\", \n",
    "    \"sh (g/kg)\",\n",
    "    \"rho (g/m**3)\",\n",
    "    \"Wx\",\n",
    "    \"Wy\",\n",
    "    \"Day sin\",\n",
    "    'Day cos',\n",
    "    'Year sin',\n",
    "    'Year cos'\n",
    "    ]\n",
    "\n",
    "    # Scale selected numeric features\n",
    "    for key in _SELECTED_NUMERIC_FEATURE_KEYS:\n",
    "        outputs[key] = tft.scale_to_0_1(outputs[key])\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YJzBQj2A_bId"
   },
   "source": [
    "Run the `Transform` component below to perform the transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-XKcRpE1HWXA"
   },
   "outputs": [],
   "source": [
    "# Ignore TF warning messages\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "# Instantiate the Transform component\n",
    "transform = Transform(\n",
    "    examples=example_gen.outputs['examples'],\n",
    "    schema=schema_gen.outputs['schema'],\n",
    "    module_file=os.path.abspath(_weather_transform_module_file))\n",
    "\n",
    "# Run the component\n",
    "context.run(transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2cAJknFq_kdL"
   },
   "source": [
    "As a sanity check, let's preview the results in the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8bewW2yIHWXB"
   },
   "outputs": [],
   "source": [
    "# Get the URI of the output artifact representing the transformed examples\n",
    "train_uri = os.path.join(transform.outputs['transformed_examples'].get()[0].uri, 'train')\n",
    "\n",
    "# Get the list of files in this directory (all compressed TFRecord files)\n",
    "tfrecord_filenames = [os.path.join(train_uri, name)\n",
    "                      for name in os.listdir(train_uri)]\n",
    "\n",
    "# Create a `TFRecordDataset` to read these files\n",
    "transformed_dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pgzfsBYAHWW8"
   },
   "outputs": [],
   "source": [
    "# Define a helper function to get individual examples\n",
    "def get_records(dataset, num_records):\n",
    "    '''Extracts records from the given dataset.\n",
    "    Args:\n",
    "        dataset (TFRecordDataset): dataset saved by ExampleGen\n",
    "        num_records (int): number of records to preview\n",
    "    '''\n",
    "    \n",
    "    # initialize an empty list\n",
    "    records = []\n",
    "    \n",
    "    # Use the `take()` method to specify how many records to get\n",
    "    for tfrecord in dataset.take(num_records):\n",
    "        \n",
    "        # Get the numpy property of the tensor\n",
    "        serialized_example = tfrecord.numpy()\n",
    "        \n",
    "        # Initialize a `tf.train.Example()` to read the serialized data\n",
    "        example = tf.train.Example()\n",
    "        \n",
    "        # Read the example data (output is a protocol buffer message)\n",
    "        example.ParseFromString(serialized_example)\n",
    "        \n",
    "        # convert the protocol bufffer message to a Python dictionary\n",
    "        example_dict = (MessageToDict(example))\n",
    "        \n",
    "        # append to the records list\n",
    "        records.append(example_dict)\n",
    "        \n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j6QqqL-7HWXB"
   },
   "outputs": [],
   "source": [
    "# Get 3 records from the dataset\n",
    "sample_records_xf = get_records(transformed_dataset, 3)\n",
    "\n",
    "# Print the output\n",
    "pp.pprint(sample_records_xf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bHKDZyP3CyeF"
   },
   "source": [
    "## Prepare Training Datasets from TFTransformOutput ¶\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z3uobzumDrHc"
   },
   "source": [
    "Now that you have the transformed dataset, you can process this further depending on the model you will be training. For time series data, it makes sense to group a fixed-length series of measurements and map that to the label found in a future time step. For example, 3 days of data can be used the predict the next day's humidity. You can use the [tf.data.Dataset.window()](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#window) method to implement these groupings.\n",
    "\n",
    "In this exercise, you will use data from the last 5 days to predict the temperature 12 hours into the future. \n",
    "\n",
    "* You already know that data is taken every 10 minutes so there will be 720 data points for 5 days. \n",
    "* You will also assume that drastic change is not expected within an hour so you will downsample the 720 data points to 120 by just getting the data every hour. Thus, you will use a *stride* (or step size) of 6 when getting the data points. This makes the `HISTORY_SIZE` for the feature window equal to 120. \n",
    "* For this single step prediction model (forecasting), the label for a dataset window will be the temperature 12 hours into the future.\n",
    "* By default, the `window()` method moves `size` (i.e. window size) steps at a time. For example, if you have a dataset with elements `[1, 2, 3, 4]` and you have `size=2`, then your results will look like: `[1, 2], [3, 4]`. You can reconfigure this behavior and move at smaller or larger steps with the `shift` parameter. For the same sample dataset with window `size=2` but with `shift=1`, the results will look like `[1, 2], [2,3], [3,4]` because the window is moving `1` element at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AWdS0Wn6ojBN"
   },
   "outputs": [],
   "source": [
    "# Constants to prepare the transformed data for modeling\n",
    "\n",
    "WORKING_DIR = transform.outputs['transform_graph'].get()[0].uri\n",
    "LABEL_KEY = 'T (degC)'\n",
    "OBSERVATIONS_PER_HOUR = 6\n",
    "HISTORY_SIZE = 120\n",
    "FUTURE_TARGET = 12\n",
    "BATCH_SIZE = 72\n",
    "SHIFT = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A0OhGDy6Kpig"
   },
   "source": [
    "You first need to get a wrapper to the transform graph so you can get information from it. That is done with the `tft.TFTransformOutput()` method and we can use it to extract the index of our label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OVLUFHcOKkNz"
   },
   "outputs": [],
   "source": [
    "import tensorflow_transform as tft\n",
    "\n",
    "# Get the output of the Transform component\n",
    "tf_transform_output = tft.TFTransformOutput(os.path.join(WORKING_DIR))\n",
    "\n",
    "# Get the index of the label key\n",
    "index_of_label = list(tf_transform_output.transformed_feature_spec().keys()).index(LABEL_KEY)\n",
    "print(index_of_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EOXd6zB5EfHH"
   },
   "source": [
    "Next, you will define several helper functions to extract the data from your transformed dataset and group it into windows. First, this `parse_function()` will help in getting the transformed features and rearranging them so that the label values (i.e. `T (degC)`) is at the end of the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xH1AzrowvWYZ"
   },
   "outputs": [],
   "source": [
    "def parse_function(example_proto):\n",
    "    \n",
    "    feature_spec = tf_transform_output.transformed_feature_spec()\n",
    "    \n",
    "    # Define features with the example_proto (transformed data) and the feature_spec using tf.io.parse_single_example \n",
    "    features = tf.io.parse_single_example(example_proto, feature_spec)\n",
    "    values = list(features.values())\n",
    "    values[index_of_label], values[len(features) - 1] = values[len(features) - 1], values[index_of_label]\n",
    "    \n",
    "    # Stack the values along the first axis\n",
    "    stacked_features = tf.stack(values, axis=0)\n",
    "\n",
    "    return stacked_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3im2Iky8Fo-6"
   },
   "source": [
    "Next, this function will separate the features and target values into a tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uSMcWLNhv21H"
   },
   "outputs": [],
   "source": [
    "def map_features_target(elements):\n",
    "    features = elements[:HISTORY_SIZE]\n",
    "    target = elements[-1:,-1]\n",
    "    return (features, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FdJSH2NxFzqz"
   },
   "source": [
    "Finally, you can define the dataset window with the function below. It uses the parameters defined above and also the helper functions to produce the batched feature-target mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z0ogFXcyvYvN"
   },
   "outputs": [],
   "source": [
    "def get_dataset(path):\n",
    "        \n",
    "    # Instantiate a tf.data.TFRecordDataset passing in the appropiate path\n",
    "    dataset = tf.data.TFRecordDataset(path, compression_type='GZIP')\n",
    "    \n",
    "    # Use the dataset's map method to map the parse_function\n",
    "    dataset = dataset.map(parse_function)\n",
    "    \n",
    "    # Use the window method with expected total size. Define stride and set drop_remainder to True\n",
    "    dataset = dataset.window(HISTORY_SIZE + FUTURE_TARGET, shift=SHIFT, stride=OBSERVATIONS_PER_HOUR, drop_remainder=True)\n",
    "    \n",
    "    # Use the flat_map method passing in an anonymous function that given a window returns window.batch(HISTORY_SIZE + FUTURE_TARGET)\n",
    "    dataset = dataset.flat_map(lambda window: window.batch(HISTORY_SIZE + FUTURE_TARGET))\n",
    "    \n",
    "    # Use the map method passing in the previously defined map_features_target function\n",
    "    dataset = dataset.map(map_features_target) \n",
    "    \n",
    "    # Use the batch method and pass in the appropiate batch size\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aNrRrW3jGgGe"
   },
   "source": [
    "You can now use the `get_dataset()` function on your transformed examples to produce the dataset windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0a3x1kKVANuD"
   },
   "outputs": [],
   "source": [
    "# Get the URI of the transformed examples\n",
    "working_dir = transform.outputs['transformed_examples'].get()[0].uri\n",
    "\n",
    "# Get the filename of the compressed examples\n",
    "train_tfrecord_files = os.listdir(working_dir + '/train')[0]\n",
    "\n",
    "# Full path string to the training tfrecord files\n",
    "path_to_train_tfrecord_files = os.path.join(working_dir, 'train', train_tfrecord_files)\n",
    "\n",
    "# Get the window datasets by passing the full path to the get_dataset function\n",
    "train_dataset = get_dataset(path_to_train_tfrecord_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZUtW89D9Guc7"
   },
   "source": [
    "Let's preview the resulting shapes of the data windows. If we print out the shapes of the tensors in a single batch, you'll notice that it indeed produced the required dimensions. It has 72 examples per batch where each contain 120 measurements for each of the 13 features in the transformed dataset. The target tensor shape only has one feature per example in the batch as expected (i.e. only `T (degC)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TUkVzURrAQzJ"
   },
   "outputs": [],
   "source": [
    "for features, target in train_dataset.take(1):\n",
    "    print(f'Shape of input features for a batch: {features.shape}')\n",
    "    print(f'Shape of targets for a batch: {target.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "guq2stcHHfFF"
   },
   "source": [
    "## Wrap Up\n",
    "\n",
    "In this notebook, you got to see how you may want to prepare seasonal data. It shows how you can handle periodicity and produce batches of dataset windows. You will be doing something similar in the next lab with sensor data. This time though, the measurements are taken at a much higher rate (20 Hz). The labels are also categorical so you will be handling that differently.\n",
    "\n",
    "On to the next!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "C2_W4_Colab_Lab_1_WeatherData.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "coursera": {
   "schema_names": [
    "MLEPC2W3-1",
    "MLEPC2W3-2",
    "MLEPC2W3-3",
    "MLEPC2W3-4",
    "MLEPC2W3-5"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
